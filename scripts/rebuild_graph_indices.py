#!/usr/bin/env python3
"""Normalize and rebuild graph indices generated by hugo-obsidian."""

from __future__ import annotations

import argparse
import html
import json
import posixpath
import re
import sys
from collections import defaultdict
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Set, Tuple
from urllib.parse import unquote, urlsplit

FRONT_MATTER_RE = re.compile(r"^\ufeff?---\r?\n(.*?)\r?\n---(?:\r?\n|$)", re.DOTALL)
TITLE_LINE_RE = re.compile(r"^\s*title\s*:\s*(.+?)\s*$", re.IGNORECASE)
WIKILINK_RE = re.compile(r"(?<!!)\[\[([^\[\]]+)\]\]")
MARKDOWN_LINK_RE = re.compile(r"(?<!!)\[([^\]]+)\]\(([^)]+)\)")
EXTERNAL_SCHEME_RE = re.compile(r"^[a-zA-Z][a-zA-Z0-9+.-]*:")


def canonicalize_id(raw: str) -> Optional[str]:
    if not isinstance(raw, str):
        return None

    value = raw.strip()
    if not value:
        return None

    parsed = urlsplit(value)
    if parsed.scheme and parsed.netloc:
        value = parsed.path

    value = value.replace("\\", "/")
    value = value.split("#", 1)[0].split("?", 1)[0]

    if not value:
        return "/"

    if not value.startswith("/"):
        value = f"/{value}"

    while "//" in value:
        value = value.replace("//", "/")

    if len(value) > 1:
        value = value.rstrip("/")

    return value or "/"


def decode_path(path: str) -> str:
    if not isinstance(path, str):
        return ""
    parts = []
    for segment in path.split("/"):
        try:
            parts.append(unquote(segment))
        except Exception:
            parts.append(segment)
    return "/".join(parts)


def default_title_from_id(page_id: str) -> str:
    if page_id == "/":
        return "/"
    segment = page_id.rsplit("/", 1)[-1]
    decoded = decode_path(segment)
    return decoded.replace("-", " ") if decoded else page_id


def extract_front_matter_title(content: str) -> Optional[str]:
    if not isinstance(content, str) or not content:
        return None

    match = FRONT_MATTER_RE.match(content)
    if not match:
        return None

    front_matter = match.group(1)
    for line in front_matter.splitlines():
        title_match = TITLE_LINE_RE.match(line)
        if not title_match:
            continue

        value = html.unescape(title_match.group(1).strip())
        if (value.startswith('"') and value.endswith('"')) or (
            value.startswith("'") and value.endswith("'")
        ):
            value = value[1:-1]

        value = value.strip()
        return value or None

    return None


def normalize_content_table(raw_content: Dict[str, object]) -> Dict[str, Dict[str, object]]:
    normalized: Dict[str, Dict[str, object]] = {}

    for raw_id, raw_entry in raw_content.items():
        page_id = canonicalize_id(raw_id)
        if not page_id:
            continue

        entry = raw_entry if isinstance(raw_entry, dict) else {}
        content = entry.get("content") if isinstance(entry.get("content"), str) else ""
        title = entry.get("title") if isinstance(entry.get("title"), str) else ""
        title = html.unescape(title).strip()

        if not title or title == "_index":
            extracted = extract_front_matter_title(content)
            if extracted:
                title = extracted

        if not title:
            title = default_title_from_id(page_id)

        normalized_entry = dict(entry)
        normalized_entry["title"] = title
        normalized_entry["content"] = content

        existing = normalized.get(page_id)
        if existing is None:
            normalized[page_id] = normalized_entry
            continue

        if (not existing.get("title") or existing.get("title") == "_index") and title:
            existing["title"] = title
        if not existing.get("content") and content:
            existing["content"] = content

        for key, value in normalized_entry.items():
            if key not in existing or existing[key] in (None, "", []):
                existing[key] = value

    return dict(sorted(normalized.items(), key=lambda item: item[0]))


def basename_keys(segment: str) -> Set[str]:
    keys: Set[str] = set()
    if not segment:
        return keys

    decoded = decode_path(segment)

    for candidate in (segment, decoded):
        candidate = candidate.strip()
        if not candidate:
            continue
        keys.add(candidate)
        if "." in candidate:
            keys.add(candidate.rsplit(".", 1)[0])

    return {key for key in keys if key}


def build_resolution_maps(
    content_table: Dict[str, Dict[str, object]]
) -> Tuple[Set[str], Dict[str, Set[str]], Dict[str, Set[str]]]:
    all_ids: Set[str] = set(content_table.keys())
    title_to_ids: Dict[str, Set[str]] = defaultdict(set)
    basename_to_ids: Dict[str, Set[str]] = defaultdict(set)

    for page_id, entry in content_table.items():
        title = entry.get("title") if isinstance(entry.get("title"), str) else ""
        title = title.strip()
        if title:
            title_to_ids[title].add(page_id)

        if page_id == "/":
            continue

        segment = page_id.rsplit("/", 1)[-1]
        for key in basename_keys(segment):
            basename_to_ids[key].add(page_id)

    return all_ids, title_to_ids, basename_to_ids


def is_internal_target(target: str) -> bool:
    if not target:
        return False

    target = target.strip()
    if not target or target.startswith("#"):
        return False

    if target.startswith("//"):
        return False

    if EXTERNAL_SCHEME_RE.match(target):
        return False

    return True


def strip_markdown_target(raw_target: str) -> str:
    target = html.unescape(raw_target.strip())

    if target.startswith("<") and target.endswith(">"):
        target = target[1:-1].strip()

    with_title = re.match(r"^(\S+)\s+['\"][^'\"]*['\"]\s*$", target)
    if with_title:
        target = with_title.group(1)

    return target.strip()


def parse_link_candidates(content: str) -> List[Tuple[str, str]]:
    candidates: List[Tuple[str, str]] = []

    for match in WIKILINK_RE.finditer(content):
        inner = match.group(1).strip()
        if not inner:
            continue

        split_alias = inner.split("|", 1)
        target_part = split_alias[0].strip()
        display = split_alias[1].strip() if len(split_alias) > 1 and split_alias[1].strip() else target_part

        target = target_part.split("#", 1)[0].strip()
        if target:
            candidates.append((target, html.unescape(display)))

    for match in MARKDOWN_LINK_RE.finditer(content):
        display = html.unescape(match.group(1).strip())
        raw_target = strip_markdown_target(match.group(2))
        target = raw_target.split("#", 1)[0].strip()
        if target:
            candidates.append((target, display or target))

    return candidates


def normalize_path_candidate(candidate: str) -> Optional[str]:
    value = candidate.strip()
    if not value:
        return None

    if value.lower().endswith(".md"):
        value = value[:-3]

    if value.endswith("/_index"):
        value = value[: -len("/_index")]
    elif value.endswith("/index"):
        value = value[: -len("/index")]

    return canonicalize_id(value)


def resolve_direct_id(source_id: str, target: str, all_ids: Set[str]) -> Optional[str]:
    source_dir = source_id.rsplit("/", 1)[0] if source_id != "/" else "/"
    if not source_dir:
        source_dir = "/"

    candidates: List[str] = []
    cleaned_target = target.strip()

    if cleaned_target.startswith("/"):
        candidates.append(cleaned_target)
    else:
        candidates.append(cleaned_target)
        candidates.append(f"/{cleaned_target.lstrip('/')}")

        relative = posixpath.normpath(posixpath.join(source_dir, cleaned_target))
        if not relative.startswith("/"):
            relative = f"/{relative}"
        candidates.append(relative)

    for candidate in candidates:
        normalized = normalize_path_candidate(candidate)
        if normalized and normalized in all_ids:
            return normalized

    return None


def resolve_target(
    source_id: str,
    target: str,
    all_ids: Set[str],
    title_to_ids: Dict[str, Set[str]],
    basename_to_ids: Dict[str, Set[str]],
) -> Optional[str]:
    direct_match = resolve_direct_id(source_id, target, all_ids)
    if direct_match:
        return direct_match

    title_matches = title_to_ids.get(target)
    if title_matches and len(title_matches) == 1:
        return next(iter(title_matches))

    basename = target.rsplit("/", 1)[-1]
    basename_match_ids: Set[str] = set()
    for key in basename_keys(basename):
        candidates = basename_to_ids.get(key)
        if candidates and len(candidates) == 1:
            basename_match_ids.update(candidates)

    if len(basename_match_ids) == 1:
        return next(iter(basename_match_ids))

    return None


def rebuild_links(content_table: Dict[str, Dict[str, object]]) -> List[Dict[str, str]]:
    all_ids, title_to_ids, basename_to_ids = build_resolution_maps(content_table)

    links: List[Dict[str, str]] = []
    seen_pairs: Set[Tuple[str, str]] = set()

    for source_id in sorted(content_table.keys()):
        raw_content = content_table[source_id].get("content")
        markdown = raw_content if isinstance(raw_content, str) else ""
        if not markdown:
            continue

        for raw_target, display in parse_link_candidates(markdown):
            target = html.unescape(raw_target).strip()
            if not is_internal_target(target):
                continue

            resolved = resolve_target(source_id, target, all_ids, title_to_ids, basename_to_ids)
            if not resolved or resolved == source_id:
                continue

            pair = (source_id, resolved)
            if pair in seen_pairs:
                continue

            seen_pairs.add(pair)
            links.append(
                {
                    "source": source_id,
                    "target": resolved,
                    "text": display or target,
                }
            )

    links.sort(key=lambda link: (link["source"], link["target"], link.get("text", "")))
    return links


def build_link_index(links: Iterable[Dict[str, str]]) -> Dict[str, object]:
    outgoing: Dict[str, List[Dict[str, str]]] = defaultdict(list)
    incoming: Dict[str, List[Dict[str, str]]] = defaultdict(list)

    for link in links:
        source = link["source"]
        target = link["target"]
        outgoing[source].append(link)
        incoming[target].append(link)

    sorted_outgoing = {
        key: sorted(value, key=lambda link: (link["target"], link.get("text", "")))
        for key, value in sorted(outgoing.items(), key=lambda item: item[0])
    }
    sorted_incoming = {
        key: sorted(value, key=lambda link: (link["source"], link.get("text", "")))
        for key, value in sorted(incoming.items(), key=lambda item: item[0])
    }

    return {
        "index": {
            "links": sorted_outgoing,
            "backlinks": sorted_incoming,
        },
        "links": list(links),
    }


def read_json(path: Path) -> object:
    with path.open("r", encoding="utf-8") as file:
        return json.load(file)


def write_json(path: Path, payload: object) -> None:
    with path.open("w", encoding="utf-8", newline="\n") as file:
        json.dump(payload, file, ensure_ascii=False, indent=2)
        file.write("\n")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Normalize and rebuild graph index assets.")
    parser.add_argument(
        "--content-index",
        default="assets/indices/contentIndex.json",
        help="Path to contentIndex.json",
    )
    parser.add_argument(
        "--link-index",
        default="assets/indices/linkIndex.json",
        help="Path to linkIndex.json",
    )
    return parser.parse_args()


def main() -> int:
    args = parse_args()
    content_index_path = Path(args.content_index)
    link_index_path = Path(args.link_index)

    if not content_index_path.exists():
        raise FileNotFoundError(f"Missing content index: {content_index_path}")
    if not link_index_path.exists():
        raise FileNotFoundError(f"Missing link index: {link_index_path}")

    raw_content = read_json(content_index_path)
    _ = read_json(link_index_path)

    if not isinstance(raw_content, dict):
        raise ValueError("contentIndex.json must contain an object at top level.")

    normalized_content = normalize_content_table(raw_content)
    rebuilt_links = rebuild_links(normalized_content)
    rebuilt_link_index = build_link_index(rebuilt_links)

    write_json(content_index_path, normalized_content)
    write_json(link_index_path, rebuilt_link_index)

    return 0


if __name__ == "__main__":
    try:
        raise SystemExit(main())
    except Exception as exc:
        print(f"[rebuild_graph_indices] {exc}", file=sys.stderr)
        raise
